{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299831e6485829a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical VIII\n",
    "Carel van Niekerk & Hsien-Chin Lin\n",
    "\n",
    "8-12.12.2025\n",
    "\n",
    "---\n",
    "\n",
    "In this practical we will implement the learning rate scheduler as well as initialise the optimiser to train our transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3becedbe2f606",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Learning rate scheduler\n",
    "\n",
    "The learning rate scheduler is used to adjust the learning rate during training. The learning rate scheduler is called every training step and returns the learning rate for that step. In a transformer model the learning rate scheduler is important because the model is trained for a long time and the learning rate needs to be adjusted to ensure that the model converges to a good solution.\n",
    "\n",
    "### 2. Optimiser\n",
    "\n",
    "In this practical we will use the AdamW optimiser. The AdamW optimiser is a variant of the Adam optimiser that uses weight decay to regularise the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20b8711fe743b8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. Study the learning rate scheduler used in the paper [Attention is all you need](https://arxiv.org/abs/1706.03762).\n",
    "2. Implement the learning rate scheduler. It is important that your scheduler class has the same interface as the pytorch learning rate scheduler classes, that is, it should have a `step()` method that updates the learning rate and a `get_lr()` method that returns the learning rate for the current step.\n",
    "3. Study the AdamW optimiser used. Write down the update equations and explain the reasoning behind the bias correction and decoupled weight decay.\n",
    "4. Initialise a AdamW optimiser for your transformer model. It is important to not use weight decay on the bias and layer normalisation parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a134f11",
   "metadata": {},
   "source": [
    "## Exercise 3: AdamW Optimizer\n",
    "\n",
    "### Update Equations\n",
    "\n",
    "Given parameters $\\theta$, gradients $g_t$, learning rate $\\alpha$, weight decay $\\lambda$, and momentum parameters $\\beta_1, \\beta_2$:\n",
    "\n",
    "**1. Compute biased first moment estimate:**\n",
    "$$m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t$$\n",
    "\n",
    "**2. Compute biased second moment estimate:**\n",
    "$$v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2$$\n",
    "\n",
    "**3. Bias correction:**\n",
    "$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
    "\n",
    "**4. Parameter update with decoupled weight decay:**\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\left( \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\lambda \\cdot \\theta_{t-1} \\right)$$\n",
    "\n",
    "### Bias Correction Reasoning\n",
    "\n",
    "At initialization, $m_0 = 0$ and $v_0 = 0$. In early steps, both estimates are **biased toward zero**:\n",
    "- After step 1: $m_1 = (1-\\beta_1) g_1$ (much smaller than true mean)\n",
    "- The bias is especially severe for $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$\n",
    "\n",
    "The correction factor $(1 - \\beta^t)$ compensates for this:\n",
    "- At $t=1$: divides by $(1 - 0.9) = 0.1$, scaling up 10x\n",
    "- As $t \\to \\infty$: $(1 - \\beta^t) \\to 1$, no correction needed\n",
    "\n",
    "### Decoupled Weight Decay Reasoning\n",
    "\n",
    "**Original Adam (L2 regularization):** Adds $\\lambda \\theta$ to the gradient, then applies adaptive scaling:\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t + \\lambda \\theta_{t-1}}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n",
    "\n",
    "**Problem:** The weight decay is scaled by $1/\\sqrt{\\hat{v}_t}$, making it inconsistent across parameters with different gradient magnitudes.\n",
    "\n",
    "**AdamW (decoupled):** Applies weight decay directly to parameters, **after** the adaptive update:\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} - \\alpha \\lambda \\theta_{t-1}$$\n",
    "\n",
    "**Benefits:**\n",
    "1. Weight decay is consistent regardless of gradient history\n",
    "2. Equivalent to true L2 regularization in SGD\n",
    "3. Better generalization in practice\n",
    "4. Hyperparameter $\\lambda$ has consistent meaning across optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91fae10",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
