{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299831e6485829a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical III\n",
    "Carel van Niekerk & Hsien-Chin Lin\n",
    "\n",
    "27-31.10.2025\n",
    "\n",
    "---\n",
    "\n",
    "In previous practicals, we delved into the attention mechanism, which serves as the foundation of transformer-style models. We noted that such mechanisms necessitate the representation of text as numerical vectors. In this session, we will investigate word tokenizers, which are methods for converting words into meaningful subword units termed as 'tokens'. Specifically, we will implement a basic Byte Pair Encoding (BPE) tokenizer to gain insights into the workings of this kind of tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3becedbe2f606",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Tokenizers\n",
    "\n",
    "Word tokenizers are used to split text into tokens, which can be words or subwords. In this practical we investigate the BPE tokenizer. BPE is a simple algorithm that iteratively replaces the most frequent pair of characters in a text with a new character. This process is repeated until a predefined number of tokens is reached. The BPE algorithm is described in the following paper: [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c78026959a9180",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. The Byte-Pair Encoding (BPE) Tokenizer\n",
    "\n",
    "The BPE algorithm is implemented in the following steps:\n",
    "\n",
    "#### 2.1. Building the base vocabulary\n",
    "\n",
    "The base vocabulary is a set of all the characters present in the data. To obtain the base vocabulary, we first find the set of all unique words in a corpus. We then find the set of all unique characters in theses words.\n",
    "\n",
    "For example, given the following set of words:\n",
    "\n",
    "`['hug', 'pug', 'pun', 'bun', 'hugs']`\n",
    "\n",
    "The base vocabulary is:\n",
    "\n",
    "`['h', 'u', 'g', 'p', 'n', 'b', 's']`\n",
    "\n",
    "#### 2.2. Building the BPE vocabulary\n",
    "\n",
    "Once we have the base vocabulary, we learn a set of merges, these are rules indicating which characters should be merged. Each merge becomes a new token in the vocabulary. The merges are learned by iteratively finding the most frequent pair of characters in the data and merging them. This process is repeated until a predefined vocabulary size is reached.\n",
    "\n",
    "Let us assume that each of the above words has a frequency of:\n",
    "\n",
    "`{'hug': 10, 'pug': 5, 'pun': 12, 'bun': 4, 'hugs': 5}`\n",
    "\n",
    "We can now compute the co-occurrence frequencies of all tokens in the vocabulary:\n",
    "\n",
    "`{('h', 'u'): 15, ('u', 'g'): 20, ('p', 'u'): 17, ('u', 'n'): 16, ('b', 'u'): 4, ('g', 's'): 5}`\n",
    "\n",
    "We see that the characters `('u', 'g')` co-occur the most. We create the merge rule `('u', 'g')` resulting in the new token 'ug'. We can now update the vocabulary and co-occurrence frequencies to:\n",
    "\n",
    "`['h', 'u', 'g', 'p', 'n', 'b', 's', 'ug']`\n",
    "\n",
    "`{('p', 'u'): 12, ('u', 'n'): 16, ('b', 'u'): 4, ('h', 'ug'): 15, ('p', 'ug'): 5, ('ug', 's'): 5}`\n",
    "\n",
    "The next merge rule is `('u', 'n')` resulting in the token 'un'.\n",
    "\n",
    "If we stop here we obtain the vocabulary:\n",
    "\n",
    "`['h', 'u', 'g', 'p', 'n', 'b', 's', 'ug', 'un']`\n",
    "\n",
    "and the set of merge rules:\n",
    "\n",
    "`{('u', 'g'): 'ug', ('u', 'n'): 'un'}`.\n",
    "\n",
    "#### 2.3. Encoding a word\n",
    "\n",
    "Based on this vocabulary we can now encode a word. First the word, for example 'pugs', is split into characters:\n",
    "\n",
    "`['p', 'u', 'g', 's']`\n",
    "\n",
    "Then the merge rules are applied to the word (here 'u' and 'g' are combined to become 'ug'):\n",
    "`['p', 'ug', 's']`\n",
    "\n",
    "Finally, the word is encoded as a sequence of tokens:\n",
    "`['p', 'ug', 's']`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b298bb9fd5735",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. Implement the BPE tokenizer module. The module should be able to extract the vocubulary from a corpus of text.\n",
    "2. Given the corpus below train your BPE tokenizer. Use a vocabulary size of 64.\n",
    "\n",
    "```python\n",
    "[\n",
    "    \"Machine learning helps in understanding complex patterns.\",\n",
    "    \"Learning machine languages can be complex yet rewarding.\",\n",
    "    \"Natural language processing unlocks valuable insights from data.\",\n",
    "    \"Processing language naturally is a valuable skill in machine learning.\",\n",
    "    \"Understanding natural language is crucial in machine learning.\"\n",
    "]\n",
    "```\n",
    "\n",
    "3. Using the BPETokenizer implementation of Huggingface ([more info](https://pypi.org/project/tokenizers/)) train a BPE tokenizer using the above corpus. Use a vocabulary of size 295 (due to larger default base vocab of this implmentation).\n",
    "4. Tokenize the following sentence: \"Machine learning is a subset of artificial intelligence.\" using both your implementation and the Huggingface implementation\n",
    "\n",
    "### Additional Material\n",
    "- [Huggingface tutorial on BPE](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d3811fe2088cee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 64\n",
      "\n",
      "Vocabulary: {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'k': 9, 'l': 10, 'm': 11, 'n': 12, 'o': 13, 'p': 14, 'r': 15, 's': 16, 't': 17, 'u': 18, 'v': 19, 'w': 20, 'x': 21, 'y': 22, 'in': 23, 'ing': 24, 'le': 25, 'an': 26, 'ua': 27, 'al': 28, 'ar': 29, 'at': 30, 'ma': 31, 'mac': 32, 'mach': 33, 'machin': 34, 'machine': 35, 'lear': 36, 'learn': 37, 'learning': 38, 'lan': 39, 'lang': 40, 'langua': 41, 'languag': 42, 'language': 43, 'un': 44, 'er': 45, 'ding': 46, 'om': 47, 'nat': 48, 'natu': 49, 'natur': 50, 'natural': 51, 'oc': 52, 'und': 53, 'under': 54, 'unders': 55, 'underst': 56, 'understan': 57, 'understanding': 58, 'com': 59, 'comp': 60, 'comple': 61, 'complex': 62, 'pr': 63}\n",
      "\n",
      "Merge rules:\n",
      "  ('i', 'n') -> in\n",
      "  ('in', 'g') -> ing\n",
      "  ('l', 'e') -> le\n",
      "  ('a', 'n') -> an\n",
      "  ('u', 'a') -> ua\n",
      "  ('a', 'l') -> al\n",
      "  ('a', 'r') -> ar\n",
      "  ('a', 't') -> at\n",
      "  ('m', 'a') -> ma\n",
      "  ('ma', 'c') -> mac\n",
      "  ('mac', 'h') -> mach\n",
      "  ('mach', 'in') -> machin\n",
      "  ('machin', 'e') -> machine\n",
      "  ('le', 'ar') -> lear\n",
      "  ('lear', 'n') -> learn\n",
      "  ('learn', 'ing') -> learning\n",
      "  ('l', 'an') -> lan\n",
      "  ('lan', 'g') -> lang\n",
      "  ('lang', 'ua') -> langua\n",
      "  ('langua', 'g') -> languag\n",
      "  ('languag', 'e') -> language\n",
      "  ('u', 'n') -> un\n",
      "  ('e', 'r') -> er\n",
      "  ('d', 'ing') -> ding\n",
      "  ('o', 'm') -> om\n",
      "  ('n', 'at') -> nat\n",
      "  ('nat', 'u') -> natu\n",
      "  ('natu', 'r') -> natur\n",
      "  ('natur', 'al') -> natural\n",
      "  ('o', 'c') -> oc\n",
      "  ('un', 'd') -> und\n",
      "  ('und', 'er') -> under\n",
      "  ('under', 's') -> unders\n",
      "  ('unders', 't') -> underst\n",
      "  ('underst', 'an') -> understan\n",
      "  ('understan', 'ding') -> understanding\n",
      "  ('c', 'om') -> com\n",
      "  ('com', 'p') -> comp\n",
      "  ('comp', 'le') -> comple\n",
      "  ('comple', 'x') -> complex\n",
      "  ('p', 'r') -> pr\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('d:/Code/implementingtransformers')\n",
    "\n",
    "from transformer_project.tokenizer import BPETokenizer\n",
    "\n",
    "corpus = [\n",
    "    \"Machine learning helps in understanding complex patterns.\",\n",
    "    \"Learning machine languages can be complex yet rewarding.\",\n",
    "    \"Natural language processing unlocks valuable insights from data.\",\n",
    "    \"Processing language naturally is a valuable skill in machine learning.\",\n",
    "    \"Understanding natural language is crucial in machine learning.\"\n",
    "]\n",
    "\n",
    "tokenizer = BPETokenizer(vocab_size=64)\n",
    "vocab, merges = tokenizer.train(corpus)\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"\\nVocabulary:\", vocab)\n",
    "print(\"\\nMerge rules:\")\n",
    "for pair, merged in merges.items():\n",
    "    print(f\"  {pair} -> {merged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a87a3829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace BPE Vocabulary size: 135\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "hf_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "hf_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(vocab_size=295, special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "hf_tokenizer.train_from_iterator(corpus, trainer)\n",
    "\n",
    "print(\"HuggingFace BPE Vocabulary size:\", hf_tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5412536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom BPE tokenizer:\n",
      "  Tokens: ['machine', 'learning', 'i', 's', 'a', 's', 'u', 'b', 's', 'e', 't', 'o', 'f', 'ar', 't', 'i', 'f', 'i', 'c', 'i', 'al', 'in', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e']\n",
      "  Encoded: [35, 38, 8, 16, 0, 16, 18, 1, 16, 4, 17, 13, 5, 29, 17, 8, 5, 8, 2, 8, 28, 23, 17, 4, 10, 10, 8, 6, 4, 12, 2, 4]\n",
      "\n",
      "HuggingFace BPE tokenizer:\n",
      "  Tokens: ['Machine', 'learning', 'is', 'a', 's', 'u', 'b', 's', 'et', 'o', 'f', 'ar', 't', 'i', 'f', 'i', 'ci', 'al', 'in', 't', 'el', 'l', 'i', 'ge', 'n', 'c', 'e', '.']\n",
      "  Encoded: [84, 60, 66, 11, 27, 29, 12, 27, 94, 24, 16, 40, 28, 19, 16, 19, 89, 38, 34, 28, 93, 21, 19, 43, 23, 13, 15, 5]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"Machine learning is a subset of artificial intelligence.\"\n",
    "\n",
    "print(\"Custom BPE tokenizer:\")\n",
    "custom_tokens = tokenizer.tokenize(test_sentence)\n",
    "print(f\"  Tokens: {custom_tokens}\")\n",
    "print(f\"  Encoded: {tokenizer.encode(test_sentence)}\")\n",
    "\n",
    "print(\"\\nHuggingFace BPE tokenizer:\")\n",
    "hf_output = hf_tokenizer.encode(test_sentence)\n",
    "print(f\"  Tokens: {hf_output.tokens}\")\n",
    "print(f\"  Encoded: {hf_output.ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8504c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
