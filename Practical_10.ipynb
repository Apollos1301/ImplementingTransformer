{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299831e6485829a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical X\n",
    "Carel van Niekerk & Hsien-Chin Lin\n",
    "\n",
    "12-16.01.2026\n",
    "\n",
    "---\n",
    "\n",
    "In this practical we will evaluate the performance of the transformer model we trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3becedbe2f606",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Autoregressive Generation\n",
    "\n",
    "In order to generate a translation we will use the autoregressive property of the transformer model. We will use the following procedure to generate a translation:\n",
    "\n",
    "1. Encode the source sentence using the encoder.\n",
    "2. Initialize the decoder with the encoded source sentence.\n",
    "3. Generate the first token of the translation by passing the start of text token through the decoder.\n",
    "4. Pass the generated token through the decoder to generate the next token and repeat until the end of text token is generated.\n",
    "\n",
    "#### 1.1. Greedy Decoding\n",
    "\n",
    "The simplest way to generate a translation is to use greedy decoding. In greedy decoding we simply select the token with the highest probability at each step.\n",
    "\n",
    "### 2. Evaluation\n",
    "\n",
    "In order to evaluate the performance of the model we will use the BLEU score. The BLEU score is a metric that measures the similarity between two sentences. See the [huggingface evaluate documentation](https://huggingface.co/spaces/evaluate-metric/bleu) for more information on the BLEU score, as well as details on using the metric in huggingface evaluate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20b8711fe743b8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. Implement the autoregressive generation procedure described above using your transformer model. (Using greedy decoding, remember to add a maximum length to the generation procedure to prevent infinite generation.)\n",
    "2. Generate translations for the test set (or a subset of the test set) of WMT17 German-English.\n",
    "3. Evaluate the BLEU score of your model on the test set (or a subset of the test set) of WMT17 German-English.\n",
    "4. Evaluate some of the translations generated by your model. Do they make sense? What are some of the errors made by your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d11f3f0",
   "metadata": {},
   "source": [
    "## Exercise 4: Evaluation of Model Translations\n",
    "\n",
    "### Overall Performance\n",
    "The model achieves a BLEU score of **26.99** (MHA baseline) and **27.05** (GQA variant) on the WMT17 German-English test set, which is competitive for a from-scratch implementation.\n",
    "\n",
    "### Translation Quality Analysis\n",
    "\n",
    "**Excellent Translations (100 BLEU):**\n",
    "Many short, straightforward sentences are translated perfectly:\n",
    "- \"Es war eine Geste, die eine Krise beendete.\" → \"It was a gesture that ended a crisis.\" ✓\n",
    "- \"Vergessen Sie den Druck.\" → \"Forget the pressure.\" ✓\n",
    "- \"Vergessen Sie den Hype.\" → \"Forget the hype.\" ✓\n",
    "\n",
    "### Common Error Patterns\n",
    "\n",
    "**1. Repetition/Degeneration on Long Sentences:**\n",
    "Some complex sentences cause the model to enter repetitive loops:\n",
    "- Source: \"Einheimische betrauerten es als letzten Verlust in einer sich gentrifizierenden Stadt.\"\n",
    "- Prediction: \"In the early days... to be a city to be a city to be a city...\"\n",
    "- This indicates attention degradation on longer sequences, possibly due to the limited max sequence length (100 tokens).\n",
    "\n",
    "**2. Untranslated Words (OOV/Rare Terms):**\n",
    "Domain-specific or rare German words sometimes appear untranslated:\n",
    "- \"Leichnam\" (corpse) left as-is in output\n",
    "- This suggests the BPE tokenizer may split rare words into suboptimal subwords\n",
    "\n",
    "**3. Word Sense Errors:**\n",
    "- \"Koch\" (chef/cook) sometimes translated as \"kitchen\" or \"cook\" instead of \"chef\"\n",
    "- \"gezogen\" (moved) translated as \"drawn\" instead of \"moved\"\n",
    "\n",
    "**4. Structural/Syntactic Issues:**\n",
    "Complex relative clauses can be mangled:\n",
    "- \"der vor kurzem nach San Francisco gezogen ist\" \n",
    "- MHA: \"recently drawn to San Francisco\" (wrong verb, missing relative pronoun)\n",
    "- GQA: \"who recently moved to San Francisco\" (correct!)\n",
    "\n",
    "**5. Complete Failures (0 BLEU):**\n",
    "Very short or idiomatic phrases sometimes fail completely:\n",
    "- \"Baugrund im Doppelpack\" → \"Double packing\" (should be: \"Construction sites coming as a twinpack\")\n",
    "- \"Einer für alle Fälle\" → \"One case by case\" (should be: \"Something for every situation\")\n",
    "\n",
    "### Observations\n",
    "1. **GQA often outperforms MHA** on fluency, despite having fewer parameters\n",
    "2. **Short sentences** are handled very well (often perfect)\n",
    "3. **Long sentences** with complex structure are the main failure mode\n",
    "4. **Domain-specific terminology** remains challenging\n",
    "5. The model successfully learns German-English word order transformation (SOV → SVO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
