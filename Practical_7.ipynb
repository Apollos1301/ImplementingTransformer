{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299831e6485829a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical VII\n",
    "Carel van Niekerk & Hsien-Chin Lin\n",
    "\n",
    "1-5.12.2025\n",
    "\n",
    "---\n",
    "\n",
    "In this practical we will combine the word embedding layer, positional encoding layer, and encoder and decoder layers from previous practicals to implement a transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20b8711fe743b8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. Study the model in the paper [Attention is all you need](https://arxiv.org/abs/1706.03762). Write down the structure of the proposed model.\n",
    "2. Study the section on word embeddings and pay close attention to the parameter sharing. Explain the benefits of parameter sharing in the transformer model.\n",
    "3. Based on your implementations of all the components, implement a transformer model. Use the pytorch `nn.Module` class to implement the model. Your model should be configurable with the following parameters:\n",
    "    - `vocab_size`: The size of the vocabulary\n",
    "    - `d_model`: The dimensionality of the embedding layer\n",
    "    - `n_heads`: The number of heads in the multi-head attention layers\n",
    "    - `num_encoder_layers`: The number of encoder layers\n",
    "    - `num_decoder_layers`: The number of decoder layers\n",
    "    - `dim_feedforward`: The dimensionality of the feedforward layer\n",
    "    - `dropout`: The dropout probability\n",
    "    - `max_len`: The maximum length of the input sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a4e63a",
   "metadata": {},
   "source": [
    "## Exercise 1: Transformer Model Structure\n",
    "\n",
    "The transformer model from \"Attention is All You Need\" consists of:\n",
    "\n",
    "### Encoder (left side)\n",
    "1. **Input Embedding**: Converts input tokens to $d_{model}$-dimensional vectors\n",
    "2. **Positional Encoding**: Adds position information using sinusoidal functions\n",
    "3. **N Encoder Layers** (N=6 in the paper), each containing:\n",
    "   - Multi-Head Self-Attention (with residual + LayerNorm)\n",
    "   - Position-wise Feed Forward Network (with residual + LayerNorm)\n",
    "\n",
    "### Decoder (right side)\n",
    "1. **Output Embedding**: Converts output tokens to $d_{model}$-dimensional vectors (shifted right)\n",
    "2. **Positional Encoding**: Same sinusoidal encoding as encoder\n",
    "3. **N Decoder Layers** (N=6 in the paper), each containing:\n",
    "   - Masked Multi-Head Self-Attention (with residual + LayerNorm) - prevents attending to future positions\n",
    "   - Multi-Head Cross-Attention over encoder output (with residual + LayerNorm)\n",
    "   - Position-wise Feed Forward Network (with residual + LayerNorm)\n",
    "\n",
    "### Output Layer\n",
    "1. **Linear**: Projects decoder output to vocabulary size\n",
    "2. **Softmax**: Converts to probability distribution over vocabulary\n",
    "\n",
    "### Model Dimensions (base model)\n",
    "- $d_{model} = 512$\n",
    "- $d_{ff} = 2048$\n",
    "- $h = 8$ (number of attention heads)\n",
    "- $d_k = d_v = d_{model}/h = 64$\n",
    "- $N = 6$ (encoder/decoder layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72da24ba",
   "metadata": {},
   "source": [
    "## Exercise 2: Parameter Sharing Benefits\n",
    "\n",
    "In the transformer, three sets of embeddings share parameters:\n",
    "\n",
    "1. **Input embedding** (encoder)\n",
    "2. **Output embedding** (decoder)\n",
    "3. **Pre-softmax linear transformation** (output layer)\n",
    "\n",
    "All three use the **same weight matrix** $E \\in \\mathbb{R}^{V \\times d_{model}}$\n",
    "\n",
    "### Benefits of Parameter Sharing:\n",
    "\n",
    "1. **Reduced Model Size**: Instead of 3 separate matrices ($3 \\times V \\times d_{model}$ parameters), only one is needed. For $V=50000$ and $d_{model}=512$, this saves ~51M parameters.\n",
    "\n",
    "2. **Improved Generalization**: Shared embeddings force the model to learn a unified semantic space where input and output tokens have consistent representations.\n",
    "\n",
    "3. **Better Learning Signal**: The output layer gradients directly update the input embeddings, providing stronger supervision for rare words.\n",
    "\n",
    "4. **Semantic Consistency**: Words have the same meaning whether they appear in input or output, so sharing embeddings enforces this consistency.\n",
    "\n",
    "5. **Transfer Learning**: The shared embedding space makes it easier to use pre-trained embeddings or transfer the model to new tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c4575",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
